# ============================================================================
# BarberX Legal AI Suite - Advanced Data Processing & Software Tools
# ============================================================================
# Version: 4.0.0-dataprocessing
# Date: January 23, 2026
# 
# This file contains dependencies for:
# - Big Data Processing (Spark, Kafka, Arrow)
# - Advanced Analytics (DuckDB, Polars, Dask, Vaex)
# - Machine Learning (scikit-learn, XGBoost, LightGBM, CatBoost)
# - Data Quality & Validation (Great Expectations, Pandera, Soda)
# - Database Tools (SQLAlchemy, Alembic, Redis)
# - ETL & Data Pipeline (Apache Airflow, Prefect, Dagster)
#
# Install: pip install -r requirements-dataprocessing.txt
# ============================================================================

# ============================================================================
# PHASE 25: BIG DATA PROCESSING
# ============================================================================

# Apache Spark - Distributed data processing (100x faster than pandas for large datasets)
pyspark==3.5.0              # Core Spark engine
findspark==2.0.1            # Helper to initialize Spark in Python

# Apache Kafka - Real-time data streaming
kafka-python==2.0.2         # Kafka client for Python
confluent-kafka==2.3.0      # High-performance Kafka client (librdkafka-based)

# Apache Arrow - Columnar in-memory data format (zero-copy, language-agnostic)
pyarrow==14.0.2             # Arrow Python bindings (also used by Spark, Pandas)
fastparquet==2024.2.0       # Fast Parquet file format support

# DuckDB - Embedded analytical database (SQLite for analytics)
duckdb==0.9.2               # In-process OLAP database (aggregations 10-100x faster than pandas)

# Polars - Lightning-fast DataFrame library (Rust-based, 5-10x faster than pandas)
polars==0.20.2              # Modern DataFrame API with lazy evaluation
polars[all]==0.20.2         # Includes parquet, excel, timezone, etc.

# Dask - Parallel computing library (scales pandas to multiple cores/nodes)
dask==2024.1.0              # Core Dask
dask[complete]==2024.1.0    # Includes dataframe, array, bag, ML
distributed==2024.1.0       # Distributed scheduler for clusters

# Vaex - Out-of-core DataFrames (work with billion-row datasets on laptop)
vaex==4.17.0                # Lazy evaluation, memory-mapped files
vaex-core==4.17.0           # Core Vaex library
vaex-hdf5==0.14.1           # HDF5 file support (efficient large datasets)
vaex-arrow==0.14.1          # Apache Arrow integration

# Modin - Drop-in pandas replacement with automatic parallelization
modin[all]==0.26.0          # Parallel pandas on Ray or Dask
ray==2.9.0                  # Distributed computing framework (Modin backend)

# ============================================================================
# PHASE 26: MACHINE LEARNING & DATA SCIENCE
# ============================================================================

# Core ML Libraries
scikit-learn==1.4.0         # Industry-standard ML library (classification, regression, clustering)
numpy==1.26.3               # Numerical computing (required by all ML libs)
scipy==1.11.4               # Scientific computing (stats, optimization)

# Gradient Boosting Frameworks (State-of-the-art for tabular data)
xgboost==2.0.3              # Extreme Gradient Boosting (winner of many Kaggle competitions)
lightgbm==4.2.0             # Microsoft's Light GBM (faster than XGBoost)
catboost==1.2.2             # Yandex's CatBoost (best for categorical features)

# Model Interpretability & Explainability
shap==0.44.0                # SHapley Additive exPlanations (explain any ML model)
lime==0.2.0.1               # Local Interpretable Model-agnostic Explanations
eli5==0.13.0                # Another explainability library (feature importance, etc.)

# Hyperparameter Optimization
optuna==3.5.0               # State-of-the-art hyperparameter tuning (better than grid search)
hyperopt==0.2.7             # Bayesian optimization for hyperparameters
scikit-optimize==0.9.0      # Sequential model-based optimization

# Experiment Tracking & Model Management
mlflow==2.9.2               # Track experiments, package models, deploy
wandb==0.16.2               # Weights & Biases (cloud-based experiment tracking)
neptune-client==1.9.1       # Neptune.ai experiment tracking

# AutoML (Automated Machine Learning)
h2o==3.44.0.3               # H2O.ai AutoML platform
auto-sklearn==0.15.0        # Automated sklearn pipeline optimization
tpot==0.12.1                # Tree-based Pipeline Optimization Tool

# Deep Learning (if needed for advanced cases)
# tensorflow==2.15.0        # Google's TensorFlow (uncomment if needed)
# torch==2.1.2              # PyTorch (already in local-ai requirements)
# keras==2.15.0             # High-level neural networks API

# Feature Engineering
featuretools==1.30.0        # Automated feature engineering
category_encoders==2.6.3    # Categorical encoding methods
imbalanced-learn==0.12.0    # Handling imbalanced datasets (SMOTE, etc.)

# ============================================================================
# PHASE 27: DATA QUALITY & VALIDATION
# ============================================================================

# Data Validation Frameworks
great-expectations==0.18.8  # Data validation, documentation, and profiling (industry standard)
pandera==0.18.0             # DataFrame validation with Pydantic-style schemas
pydantic==2.5.3             # Data validation using Python type annotations
pydantic[email]==2.5.3      # Email validation

# Schema Validation
cerberus==1.3.5             # Lightweight schema validation
jsonschema==4.20.0          # JSON schema validation
marshmallow==3.20.1         # Object serialization and validation

# Data Comparison & Diffing
deepdiff==6.7.1             # Deep comparison of data structures
data-diff==0.9.0            # Database and DataFrame diffing
diff-match-patch==20230430  # Text diffing (for document comparison)

# Data Quality Monitoring
soda-core==3.3.0            # Data quality checks and monitoring
soda-core-duckdb==3.3.0     # DuckDB connector for Soda
soda-core-pandas==3.3.0     # Pandas connector for Soda

# Data Profiling
pandas-profiling==3.6.6     # Generate profile reports from pandas DataFrames
ydata-profiling==4.6.4      # Next-gen profiling (formerly pandas-profiling)
dataprep==0.4.5             # Data profiling and cleaning

# Data Cleaning
pyjanitor==0.26.0           # Clean APIs for data cleaning
scrubadub==2.0.1            # Remove PII from text
faker==22.2.0               # Generate fake data for testing/anonymization

# ============================================================================
# PHASE 28: DATABASE & STORAGE TOOLS
# ============================================================================

# SQL Databases
sqlalchemy==2.0.25          # SQL toolkit and ORM (industry standard)
alembic==1.13.1             # Database migration tool (works with SQLAlchemy)
psycopg2-binary==2.9.9      # PostgreSQL adapter
pymysql==1.1.0              # MySQL adapter
cx-Oracle==8.3.0            # Oracle database adapter

# NoSQL Databases
redis==5.0.1                # Redis Python client (caching, message broker)
pymongo==4.6.1              # MongoDB Python driver
motor==3.3.2                # Async MongoDB driver (for async Python)

# Graph Databases
neo4j==5.16.0               # Neo4j graph database driver
py2neo==2021.2.3            # Neo4j toolkit and ORM

# Time-Series Databases
influxdb-client==1.39.0     # InfluxDB client (time-series data)

# Database Connection Pooling
sqlalchemy-utils==0.41.1    # Utility functions for SQLAlchemy
databases==0.8.0            # Async database support

# ============================================================================
# PHASE 29: ETL & DATA PIPELINE TOOLS
# ============================================================================

# Workflow Orchestration
apache-airflow==2.8.0       # Workflow automation platform (industry standard)
apache-airflow-providers-postgres==5.10.0
apache-airflow-providers-redis==3.6.0
celery==5.3.4               # Distributed task queue (used by Airflow)

# Modern Workflow Tools
prefect==2.14.21            # Modern data workflow orchestration (simpler than Airflow)
dagster==1.6.2              # Data orchestrator for machine learning
dagster-postgres==0.22.2    # PostgreSQL connector for Dagster
dagster-duckdb==0.22.2      # DuckDB connector for Dagster

# Message Queuing
pika==1.3.2                 # RabbitMQ client
kombu==5.3.5                # Messaging library (used by Celery)

# Job Scheduling
schedule==1.2.0             # Simple job scheduling library
apscheduler==3.10.4         # Advanced Python Scheduler

# ============================================================================
# PHASE 30: DATA TRANSFORMATION & PROCESSING
# ============================================================================

# Data Wrangling
pandas==2.1.4               # Core DataFrame library (baseline)
numpy==1.26.3               # Array operations
openpyxl==3.1.2             # Excel file support (.xlsx)
xlrd==2.0.1                 # Excel file support (.xls - legacy)
xlsxwriter==3.1.9           # Write Excel files with formatting

# CSV & Text Processing
chardet==5.2.0              # Character encoding detection
python-magic==0.4.27        # File type detection
tabulate==0.9.0             # Pretty-print tabular data

# JSON & YAML
orjson==3.9.12              # Fast JSON parser (10x faster than built-in)
ujson==5.9.0                # Ultra-fast JSON encoder/decoder
pyyaml==6.0.1               # YAML parser
ruamel.yaml==0.18.5         # Advanced YAML parser (preserves comments)

# XML Processing
lxml==5.0.0                 # XML/HTML processing
xmltodict==0.13.0           # XML to dict converter
defusedxml==0.7.1           # Secure XML parsing (prevents XXE attacks)

# Data Serialization
msgpack==1.0.7              # Binary serialization (faster than JSON)
pickle5==0.0.12             # Backport of Python 3.8+ pickle protocol
cloudpickle==3.0.0          # Extended pickling (for complex objects)
joblib==1.3.2               # Efficient serialization of large objects

# Compression
zstandard==0.22.0           # Zstandard compression (fast + high ratio)
python-snappy==0.6.1        # Snappy compression (very fast)
lz4==4.3.3                  # LZ4 compression (extremely fast)

# ============================================================================
# PHASE 31: DATA VISUALIZATION (ADVANCED)
# ============================================================================

# Already have: plotly, matplotlib, seaborn in extended requirements
# Adding advanced visualization tools

# Interactive Dashboards
streamlit==1.29.0           # Create web apps quickly
dash==2.14.2                # Plotly's dashboard framework
dash-bootstrap-components==1.5.0  # Bootstrap themes for Dash
panel==1.3.8                # Holoviz's dashboard tool

# Advanced Plotting
altair==5.2.0               # Declarative statistical visualization
bokeh==3.3.2                # Interactive web-ready plots
holoviews==1.18.1           # Data analysis and visualization
hvplot==0.9.1               # High-level plotting API built on HoloViews

# Graph Visualization (already have networkx)
pygraphviz==1.11            # Graphviz wrapper (for network diagrams)
graphviz==0.20.1            # Python Graphviz interface
pydot==1.4.2                # Graphviz's DOT language interface

# Geospatial Visualization (already have folium)
geopandas==0.14.2           # Geospatial data manipulation
contextily==1.4.0           # Basemaps for matplotlib/geopandas
plotly==5.18.0              # Already in extended, but ensuring version

# ============================================================================
# PHASE 32: STATISTICAL ANALYSIS
# ============================================================================

# Statistical Computing
statsmodels==0.14.1         # Statistical models (regression, time series, etc.)
pingouin==0.5.4             # Statistical tests (ANOVA, t-test, etc.)
scipy==1.11.4               # Already included above
lifelines==0.27.8           # Survival analysis

# Causal Inference
causalml==0.15.0            # Causal ML methods
dowhy==0.11.1               # Causal inference library (Microsoft)
econml==0.15.0              # Heterogeneous treatment effects (Microsoft)

# Time Series Analysis
prophet==1.1.5              # Facebook's time series forecasting
pmdarima==2.0.4             # Auto ARIMA for time series
tsfresh==0.20.2             # Automatic time series feature extraction
sktime==0.26.0              # Time series ML toolkit

# Bayesian Statistics
pymc==5.10.3                # Probabilistic programming in Python
arviz==0.17.0               # Bayesian visualization

# ============================================================================
# PHASE 33: TEXT PROCESSING (ADVANCED)
# ============================================================================

# Already have: spaCy, transformers, etc. in extended requirements
# Adding specialized text tools

# Text Extraction & Parsing
textract==1.6.5             # Extract text from any document
pdfplumber==0.10.3          # PDF text extraction (better than PyPDF2)
pypdf==3.17.4               # PDF manipulation (merger, splitter)
python-docx==1.1.0          # Word document manipulation

# Text Cleaning & Normalization
ftfy==6.1.3                 # Fix unicode text
unidecode==1.3.8            # ASCII transliteration
contractions==0.1.73        # Expand contractions ("don't" â†’ "do not")
texthero==1.1.0             # Text preprocessing pipeline

# Natural Language Processing
textblob==0.17.1            # Simple NLP (sentiment, POS tagging)
nltk==3.8.1                 # Natural Language Toolkit
gensim==4.3.2               # Topic modeling, doc2vec, word2vec
fuzzywuzzy==0.18.0          # Fuzzy string matching
python-Levenshtein==0.24.0  # Fast string distance calculations
rapidfuzz==3.6.1            # Faster fuzzy matching (replacement for fuzzywuzzy)

# Regular Expressions (advanced)
regex==2023.12.25           # Better regex than built-in re module

# ============================================================================
# PHASE 34: WEB SCRAPING & DATA COLLECTION
# ============================================================================

# HTTP Clients
requests==2.31.0            # HTTP library (standard)
httpx==0.26.0               # Async HTTP client
urllib3==2.1.0              # HTTP client with pooling
aiohttp==3.9.1              # Async HTTP client/server

# Web Scraping
beautifulsoup4==4.12.2      # HTML/XML parsing
lxml==5.0.0                 # Already included above
scrapy==2.11.0              # Web crawling framework
selenium==4.16.0            # Browser automation
playwright==1.40.0          # Modern browser automation (better than Selenium)

# API Clients
google-api-python-client==2.112.0  # Google APIs
tweepy==4.14.0              # Twitter/X API
praw==7.7.1                 # Reddit API

# Rate Limiting & Caching
ratelimit==2.2.1            # Rate limiting decorator
cachetools==5.3.2           # Caching utilities
requests-cache==1.1.1       # HTTP request caching

# ============================================================================
# PHASE 35: MONITORING & LOGGING
# ============================================================================

# Logging
loguru==0.7.2               # Better logging (color, rotation, etc.)
structlog==24.1.0           # Structured logging
python-json-logger==2.0.7   # JSON-formatted logs

# Monitoring & Metrics
prometheus-client==0.19.0   # Prometheus metrics
statsd==4.0.1               # StatsD metrics client
sentry-sdk==1.39.2          # Error tracking (Sentry)

# Performance Profiling
py-spy==0.3.14              # Sampling profiler (no code changes needed)
memory-profiler==0.61.0     # Line-by-line memory usage
line-profiler==4.1.1        # Line-by-line time profiler
scalene==1.5.38             # High-performance CPU/GPU/memory profiler

# System Monitoring
psutil==5.9.7               # System and process utilities
gputil==1.4.0               # GPU monitoring

# ============================================================================
# PHASE 36: TESTING & QUALITY ASSURANCE
# ============================================================================

# Testing Frameworks
pytest==7.4.4               # Testing framework (industry standard)
pytest-cov==4.1.0           # Coverage plugin for pytest
pytest-asyncio==0.23.3      # Async support for pytest
pytest-mock==3.12.0         # Mocking plugin
pytest-xdist==3.5.0         # Parallel test execution

# Property-Based Testing
hypothesis==6.92.2          # Property-based testing (generates test cases)

# Load Testing
locust==2.20.0              # Load testing framework
faker==22.2.0               # Already included above (generate fake data)

# Mocking & Stubbing
responses==0.24.1           # Mock HTTP requests
vcrpy==5.1.0                # Record and replay HTTP interactions
freezegun==1.4.0            # Mock datetime

# Code Quality
pylint==3.0.3               # Python linter
flake8==7.0.0               # Style guide enforcement
black==23.12.1              # Code formatter
isort==5.13.2               # Import sorter
mypy==1.8.0                 # Static type checker
bandit==1.7.6               # Security linter

# ============================================================================
# PHASE 37: UTILITIES & HELPERS
# ============================================================================

# Date & Time
python-dateutil==2.8.2      # Date/time utilities
arrow==1.3.0                # Better datetime library
pendulum==3.0.0             # Modern datetime library with timezone support
pytz==2023.3                # Timezone library

# Configuration Management
python-dotenv==1.0.0        # Load environment variables from .env
pydantic-settings==2.1.0    # Settings management with Pydantic
dynaconf==3.2.4             # Layered configuration (env, file, etc.)

# Retry & Backoff
tenacity==8.2.3             # Retry library with exponential backoff
backoff==2.2.1              # Function decoration for backoff

# Progress Bars
tqdm==4.66.1                # Progress bar library
rich==13.7.0                # Rich text and beautiful formatting

# File Operations
pathlib==1.0.1              # Object-oriented filesystem paths (built-in, but ensuring)
watchdog==3.0.0             # File system event monitoring
aiofiles==23.2.1            # Async file operations

# CLI Tools
click==8.1.7                # Command-line interface creation
typer==0.9.0                # Modern CLI framework (built on Click)
questionary==2.0.1          # Interactive command-line prompts

# Hashing & Checksums
xxhash==3.4.1               # Extremely fast hash algorithm
hashlib==1.0.2              # Standard hash library (built-in)

# ============================================================================
# DEPENDENCY NOTES & INSTALLATION INSTRUCTIONS
# ============================================================================

# TOTAL PACKAGES: 200+ dependencies across 13 new phases
#
# INSTALLATION OPTIONS:
#
# 1. Install everything (WARNING: Large download ~5-10 GB):
#    pip install -r requirements-dataprocessing.txt
#
# 2. Install by phase (recommended):
#    # Big data only
#    pip install pyspark kafka-python pyarrow duckdb polars dask vaex modin ray
#    
#    # ML only
#    pip install scikit-learn xgboost lightgbm catboost shap optuna mlflow
#    
#    # Data quality only
#    pip install great-expectations pandera pydantic soda-core ydata-profiling
#
# 3. Install core + selective:
#    pip install pandas polars duckdb scikit-learn great-expectations
#
# ============================================================================
# SYSTEM REQUIREMENTS
# ============================================================================
#
# Minimum:
#   - 16 GB RAM (for basic data processing)
#   - 4 CPU cores
#   - 20 GB free disk space
#
# Recommended:
#   - 32 GB RAM (for Spark, large ML models)
#   - 8+ CPU cores
#   - 100 GB free disk space
#   - SSD for database/cache
#
# Optimal:
#   - 64+ GB RAM (for big data processing)
#   - 16+ CPU cores
#   - 500 GB+ SSD
#   - GPU for deep learning (optional)
#
# ============================================================================
# COMPATIBILITY
# ============================================================================
#
# Python Version: 3.10+
# Operating Systems: Windows, Linux, macOS
# 
# Some packages require compilation:
#   - Linux: `apt install build-essential python3-dev`
#   - macOS: `xcode-select --install`
#   - Windows: Install Visual Studio Build Tools
#
# ============================================================================
# EXTERNAL DEPENDENCIES
# ============================================================================
#
# Apache Spark requires Java:
#   - Java 11 or 17 (OpenJDK recommended)
#   - Set JAVA_HOME environment variable
#
# Apache Kafka (if running locally):
#   - Download from: https://kafka.apache.org/downloads
#   - Requires Zookeeper
#
# Redis (if using caching):
#   - Linux: `apt install redis-server`
#   - macOS: `brew install redis`
#   - Windows: Use WSL or Docker
#
# PostgreSQL (if using as database):
#   - Download from: https://www.postgresql.org/download/
#
# Graphviz (for graph visualization):
#   - Linux: `apt install graphviz`
#   - macOS: `brew install graphviz`
#   - Windows: Download from graphviz.org
#
# ============================================================================
# PERFORMANCE BENCHMARKS (Approximate)
# ============================================================================
#
# Data Processing Speed (10M rows):
#   - pandas: 60 seconds
#   - polars: 6 seconds (10x faster)
#   - duckdb: 3 seconds (20x faster)
#   - pyspark: 15 seconds (4x faster, but scales to billions)
#
# ML Training (1M rows, 100 features):
#   - scikit-learn: 5 minutes
#   - xgboost: 2 minutes
#   - lightgbm: 1 minute
#   - catboost: 3 minutes (but best accuracy)
#
# ============================================================================
# VERSION CONTROL
# ============================================================================
#
# This file should be committed to version control
# Use `pip freeze > requirements-frozen.txt` to lock exact versions
#
# ============================================================================
